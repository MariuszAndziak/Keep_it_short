{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroids - most relevant tokens; tokens that contain the same meaning\n",
    "1. Sum up vector representation of words that are part of a centroid => get embedding representation of the centroid.\n",
    "2. Every sentence is scored (cosine similarity) based on how similar they are to the centroid embedding.\n",
    "3. Select sentences based on their score until a certain number of words (hyperparameter) is reached\n",
    "4. Avoid redundancy - if a chosen sentence is too similar to the ones in the already produced summary, don't add it (cosine similarity + predefined threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aclanthology.org/W17-1003.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1707.02268v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News headlines\n",
    "\n",
    "Web snippets from search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Some sample text containing punctuation. This is just an example - for testing. Nothing more'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = List[float]\n",
    "\n",
    "def dot(v: vector, w: vector):\n",
    "    return sum([vi * wi for vi, wi in zip(v,w)])\n",
    "\n",
    "def cos_sim(v: vector, w: vector):\n",
    "    return dot(v, w) / (dot(v,v) * dot(w,w)) ** .5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.oryg = text\n",
    "\n",
    "    def lower(self):\n",
    "        self.text = self.text.lower() \n",
    "        return self.text\n",
    "    \n",
    "    def remove_punctuation(self):\n",
    "        self.text = self.text.translate(self.text.maketrans('', '', string.punctuation))\n",
    "        return self.text \n",
    "    \n",
    "    def remove_stop_words(self):\n",
    "        self.text = ' '.join([word for word in self.text.split() if word not in STOP_WORDS])\n",
    "        return self.text\n",
    "    \n",
    "    def remove_digits(self):\n",
    "        self.text = re.sub(r'[\\d+]', '', self.text)\n",
    "        return self.text\n",
    "    \n",
    "    def basic_pipeline(self):\n",
    "        self.lower()\n",
    "        self.remove_digits()\n",
    "        self.remove_punctuation()\n",
    "        self.remove_stop_words()\n",
    "        return self.text\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text containing punctuation example testing nothing'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = Preprocessing(text)\n",
    "cleaned_text.basic_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(cleaned_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Pipeline([\n",
    "    ('count', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(norm = None, sublinear_tf = False, smooth_idf = False))\n",
    "]).fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5dc7221bd57f443bd21d92d893b3958d3081f94c6c945d5f95188cb4cde5b4f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
